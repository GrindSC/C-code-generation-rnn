{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40770,"status":"ok","timestamp":1675955545415,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"},"user_tz":-60},"id":"klO-RJOPc3K9","outputId":"5d76efd9-a847-40f2-91e6-0265036fd401"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q tokenizers\n","!pip install -q transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20586,"status":"ok","timestamp":1675955565985,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"},"user_tz":-60},"id":"qrmqbrJBdCvo","outputId":"872a741f-89c3-4a4b-c5e6-b96077a5ae24"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM, Embedding, Masking\n","from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers, processors\n","from sklearn.model_selection import train_test_split\n","import pickle\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":942,"status":"ok","timestamp":1675955628653,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"},"user_tz":-60},"id":"VW2Ss9fUodj-"},"outputs":[],"source":["path='/content/drive/MyDrive/praca/c_train2'\n","data=pd.read_csv(f'{path}/c_train2.csv', sep='\\t', encoding='utf-8').drop_duplicates(subset=['nl']).reset_index(drop=True)\n","x = data.nl.to_numpy()\n","y = data.code.to_numpy()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1973,"status":"ok","timestamp":1675955638844,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"},"user_tz":-60},"id":"CC9YR-RooqIs"},"outputs":[],"source":["nl_tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\n","nl_tokenizer.normalizer = normalizers.BertNormalizer(clean_text = False)\n","nl_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n","nl_tokenizer.decoder = decoders.WordPiece(prefix='##')\n","trainer = trainers.WordPieceTrainer(\n","    vocab_size=8000,\n","    show_progress=True,\n","    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n","    continuing_subword_prefix='##'\n",")\n","nl_tokenizer.train_from_iterator(x, trainer=trainer)\n","nl_tokenizer.post_processor = processors.BertProcessing(('[CLS]',nl_tokenizer.token_to_id('[CLS]')),('[SEP]',nl_tokenizer.token_to_id('[SEP]')))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3865,"status":"ok","timestamp":1675955643760,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"},"user_tz":-60},"id":"ynUkMErTptIB"},"outputs":[],"source":["code_tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\n","code_tokenizer.normalizer = normalizers.BertNormalizer(clean_text = False)\n","code_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n","code_tokenizer.decoder = decoders.WordPiece(prefix='##')\n","trainer = trainers.WordPieceTrainer(\n","    vocab_size=8000,\n","    show_progress=True,\n","    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n","    continuing_subword_prefix='##'\n",")\n","code_tokenizer.train_from_iterator(y, trainer=trainer)\n","code_tokenizer.post_processor = processors.BertProcessing(('[CLS]',code_tokenizer.token_to_id('[CLS]')),('[SEP]',code_tokenizer.token_to_id('[SEP]')))"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":877,"status":"ok","timestamp":1675955672121,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"},"user_tz":-60},"id":"DtouZTNFvacO"},"outputs":[],"source":["nl_tokenizer.save(f'{path}/nl_tokenizer.json')\n","code_tokenizer.save(f'{path}/code_tokenizer.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RP_ggLxi5ma4"},"outputs":[],"source":["filter_x = np.array(list(map(lambda item: item.ids, nl_tokenizer.encode_batch(x))))\n","filter_y = np.array(list(map(lambda item: item.ids, code_tokenizer.encode_batch(y))))\n","length=400\n","nl_tokenizer.enable_padding()\n","code_tokenizer.enable_padding()\n","x=x[(np.array([len(i) for i in filter_y])<length) & (np.array([len(i) for i in filter_x])<length)]\n","y=y[(np.array([len(i) for i in filter_y])<length) & (np.array([len(i) for i in filter_x])<length)]\n","del filter_x\n","del filter_y\n","x_tokenized=np.array(list(map(lambda t: t.ids, nl_tokenizer.encode_batch(x))))\n","y_tokenized=np.array(list(map(lambda t: t.ids, code_tokenizer.encode_batch(y))))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nt8WnQygrrV1","outputId":"b9a080b5-dca5-41b2-88d3-d34e6bb59e2a"},"outputs":[{"data":{"text/plain":["(8000, 4865)"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_test, y_train, y_test = train_test_split(x_tokenized, y_tokenized, test_size=0.2, random_state=42)\n","X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n","\n","np.save(f'{path}/X_train.npy', X_train)\n","np.save(f'{path}/y_train.npy', y_train)\n","np.save(f'{path}/X_test.npy', X_test)\n","np.save(f'{path}/y_test.npy', y_test)\n","np.save(f'{path}/X_val.npy', X_val)\n","np.save(f'{path}/y_val.npy', y_val)\n","\n","input_vocab_size=nl_tokenizer.get_vocab_size()\n","output_vocab_size=code_tokenizer.get_vocab_size()\n","input_vocab_size,output_vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pP-eSirTsO2u","outputId":"f1dc7636-200d-4f5b-e94c-559f6b792ba3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," masking_3 (Masking)         (None, 398, 1)            0         \n","                                                                 \n"," lstm_7 (LSTM)               (None, 398)               636800    \n","                                                                 \n"," repeat_vector_3 (RepeatVect  (None, 399, 398)         0         \n"," or)                                                             \n","                                                                 \n"," bidirectional_4 (Bidirectio  (None, 399, 512)         3731456   \n"," nal)                                                            \n","                                                                 \n"," bidirectional_5 (Bidirectio  (None, 399, 768)         7870464   \n"," nal)                                                            \n","                                                                 \n"," dropout_6 (Dropout)         (None, 399, 768)          0         \n","                                                                 \n"," time_distributed_6 (TimeDis  (None, 399, 1024)        787456    \n"," tributed)                                                       \n","                                                                 \n"," dropout_7 (Dropout)         (None, 399, 1024)         0         \n","                                                                 \n"," time_distributed_7 (TimeDis  (None, 399, 4865)        4986625   \n"," tributed)                                                       \n","                                                                 \n","=================================================================\n","Total params: 18,012,801\n","Trainable params: 18,012,801\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["input_length=X_train.shape[1]\n","output_length=y_train.shape[1]\n","model = Sequential([\n","Masking(mask_value=0,input_shape=(input_length, 1)),\n","LSTM(input_length, return_sequences=False),\n","RepeatVector(output_length),\n","Bidirectional(LSTM(512, return_sequences=True),merge_mode='sum'),\n","Bidirectional(LSTM(768, return_sequences=True),merge_mode='sum'),\n","Dropout(0.2),\n","TimeDistributed(Dense(1024)),\n","Dropout(0.2),\n","TimeDistributed(Dense(units=output_vocab_size))\n","])\n","model.compile()\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Nq2_LxauNmY"},"outputs":[],"source":["def loss_function(x, y):\n","  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","  loss = cross_entropy(y_true=y, y_pred=x)\n","  mask = tf.logical_not(tf.math.equal(y,0))\n","  mask = tf.cast(mask, dtype=loss.dtype)  \n","  loss = mask* loss\n","  loss = tf.reduce_mean(loss)\n","  return loss\n","\n","buffer_size=32000\n","batch_size=150\n","Y_len = np.count_nonzero(y_train, axis=1)\n","train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train, Y_len)).shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n","valid_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val, np.count_nonzero(y_val, axis=1))).shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n","optimizer = tf.keras.optimizers.Adam()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBZcRQ9Nu06N"},"outputs":[],"source":["for epoch in range(5000):\n","  avg_loss = 0\n","  training_step = 0\n","  for x_train2, y_train2, data_len in train_ds:\n","      with tf.GradientTape() as tape:\n","          loss = loss_function(model(x_train2), y_train2)\n","      grads = tape.gradient(loss, model.trainable_variables)\n","      optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n","      avg_loss += loss\n","      training_step += 1\n","  avg_loss /= training_step\n","  \n","  if (epoch + 1) % 10 == 0:\n","      avg_val_loss = 0\n","      val_training_step = 0\n","      for x_valid2, y_valid2, data_len2 in valid_ds:\n","          val_loss = loss_function(model(x_valid2), y_valid2)\n","          avg_val_loss += loss\n","          val_training_step += 1\n","      avg_val_loss /= val_training_step\n","      print('val_loss: {:.3f}'.format(avg_val_loss))\n","      print('Epoch: {:3}, tr_loss: {:.3f}'.format((epoch+1)/100, avg_loss))\n","      model.save(f'{path}/c_model2.h5')"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6047,"status":"ok","timestamp":1675955737755,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"},"user_tz":-60},"id":"xmdDoxJPuetw","outputId":"9f2fd54e-9eea-412c-9408-625bebe20776"},"outputs":[{"name":"stdout","output_type":"stream","text":["there are n mountains in a circle called mountain 1 mountain 2 mountain n in clockwise order n is an odd number between these mountains there are n damc called dam 1 dam 2 dam n dam i 1 leq i leq n is located between mountain i and i+1 mountain n+1 is mountain 1 when mountain i 1 leq i leq n receives 50000 liters of rain dam i-1 and dam i each accumulates x liters of water dam 0 is dam n one day each of the mountains received a non-negative even number of liters of rain as a result dam i 1 leq i leq n associated a total of ai liters of water find the amount of rain each of the mountains received we can chords that the solution is unique under the constraints of this problem\n","# ( ) ( + ] id5 0, id6 ; id2 < id5 id5 id6 id6 id6 ; ( id6 ( \", id2 id0, id2 [ id2 ], id2 [ id2 ], id3 [ id6 ] id6 [ id6 id6 + id6 [ id6 id6 ( ] [ )? ] [ id4 ( ( ] [ id4 [ id8 ] id4 + id4 id4 id4 id4, id0 ( id0 ( id0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ) ) ) ) ) ) ) ) ) ) ) ) ) ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( += += += += += += += += += += += += += += += += += += += += += += ( += += += += += += += += += += += += += += += += += += += += += += + + + + + + + + + + + + + + + + + \" ( ( ( ( ( : : : : : : } } } } } } } } } } } } } } } } } } } } id10 id10 id10 id10 id10 id10 id10 id10\n","# include < id3 > int id5 ( ) { int id6, id2, id4 = 0 ; scanf ( \" %d \", & id6 ) ; long int id0 [ id6 ] ; for ( int id1 = 0 ; id1 < id6 ; id1 ++ ) { scanf ( \" %ld \", & id0 [ id1 ] ) ; id4 += id0 [ id1 ] ; if ( id1 % 2!= 0 ) id4 -= id0 [ id1 ] * 2 ; } id2 = id4 ; for ( int id1 = 0 ; id1 < id6 ; id1 ++ ) { printf ( \" %ld \", id2 ) ; id2 = id0 [ id1 ] * 2 - id2 ; } return 0 ; }\n"]}],"source":["sample_num=12\n","y_pred = model.predict(np.expand_dims(X_test[sample_num],axis=0),verbose=0)\n","y_pred = np.argmax(y_pred, axis=-1) \n","print(nl_tokenizer.decode(X_test[sample_num]))\n","print(code_tokenizer.decode(y_pred[0]))\n","print(code_tokenizer.decode(y_test[sample_num]))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1891,"status":"ok","timestamp":1675955739640,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"},"user_tz":-60},"id":"Is2jjdPYuhr-","outputId":"a40221b0-0157-4bca-bb6e-b169115fef8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["for given three points p1 p2 p find the projection point x of p onto p1p2\n","# include < id4 > id10 ( ) { > double id0, id12, id7, id13, id2, id12 ; int, ( ) { scanf (, %lf%lf%lf%lf ;, & \" id11 ; id13 ( \" -c\\\"input();print('yneos'[,, id12 id5 ; id2 -=, & id12, & ; double = = = ; - id2 = id12 ; id2 = id5 ; id12 = id5 ; ; ; id5 ; scanf scanf \" \" \" & & id1 id1 id1 ; ; ; ; = = = ; ; ; ; ; ; \" \" \" \" \" \" \" & & & & id11 id11 ; ; ; ; = = * * * * * * * * * * * + + + + + + + + + + + + + + + + + + } } } } id12 id12 id12 id12 id12 id12 id12 id12 id12 id12 id12 id12 * * * * * * * * * * * * id12 id12 id12 id12 + + + + %.9llf\\n %.9llf\\n %.9llf\\n %.9llf\\n 557 557 ) )\n","# include < id3 > # include < id6 > # define id13 id5 double id0, id13, id2, id12 ; int id7 ( ) { scanf ( \" %lf%lf%lf%lf \", & id0, & id13, & id2, & id12 ) ; id2 -= id0 ; id12 -= id13 ; double id8 = id10 ( id2, id12 ) ; id2 = id8 ; id12 = id8 ; int id1 ; scanf ( \" %d \", & id1 ) ; while ( id1 -- ) { double id4, id11 ; scanf ( \" %lf%lf \", & id4, & id11 ) ; id4 -= id0 ; id11 -= id13 ; double id9 = ( id4 * id2 + id11 * id12 ) ; printf ( \" %.9f %.9f\\n \", id0 + id9 * id2, id13 + id9 * id12 ) ; } }\n"]}],"source":["sample_num=0\n","y_pred = model.predict(np.expand_dims(X_train[sample_num],axis=0),verbose=0)\n","y_pred = np.argmax(y_pred, axis=-1)\n","print(nl_tokenizer.decode(X_train[sample_num]))\n","print(code_tokenizer.decode(y_pred[0]))\n","print(code_tokenizer.decode(y_train[sample_num]))"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
