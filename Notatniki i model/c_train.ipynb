{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klO-RJOPc3K9","outputId":"ef81ec59-aa3b-4129-e08c-23dd62fe3eb9","executionInfo":{"status":"ok","timestamp":1675954896802,"user_tz":-60,"elapsed":43290,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q tokenizers\n","!pip install -q transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrmqbrJBdCvo","outputId":"553e7b65-8a15-433a-e829-758054b1cc6f","executionInfo":{"status":"ok","timestamp":1675954920912,"user_tz":-60,"elapsed":24114,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM, Embedding, Masking\n","from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers, processors\n","from sklearn.model_selection import train_test_split\n","import pickle\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VW2Ss9fUodj-","executionInfo":{"status":"ok","timestamp":1675954922132,"user_tz":-60,"elapsed":1223,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"}}},"outputs":[],"source":["path='/content/drive/MyDrive/praca/c_train'\n","data=pd.read_csv(f'{path}/c_train.csv', sep='\\t', encoding='utf-8').drop_duplicates(subset=['nl']).reset_index(drop=True)\n","x = data.nl.to_numpy()\n","y = data.code.to_numpy()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"CC9YR-RooqIs","executionInfo":{"status":"ok","timestamp":1675954953482,"user_tz":-60,"elapsed":3235,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"}}},"outputs":[],"source":["nl_tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\n","nl_tokenizer.normalizer = normalizers.BertNormalizer(clean_text = False)\n","nl_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n","nl_tokenizer.decoder = decoders.WordPiece(prefix='##')\n","trainer = trainers.WordPieceTrainer(\n","    vocab_size=8000,\n","    show_progress=True,\n","    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n","    continuing_subword_prefix='##'\n",")\n","nl_tokenizer.train_from_iterator(x, trainer=trainer)\n","nl_tokenizer.post_processor = processors.BertProcessing(('[CLS]',nl_tokenizer.token_to_id('[CLS]')),('[SEP]',nl_tokenizer.token_to_id('[SEP]')))"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ynUkMErTptIB","executionInfo":{"status":"ok","timestamp":1675954959235,"user_tz":-60,"elapsed":3913,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"}}},"outputs":[],"source":["code_tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\n","code_tokenizer.normalizer = normalizers.BertNormalizer(clean_text = False)\n","code_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n","code_tokenizer.decoder = decoders.WordPiece(prefix='##')\n","trainer = trainers.WordPieceTrainer(\n","    vocab_size=8000,\n","    show_progress=True,\n","    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n","    continuing_subword_prefix='##'\n",")\n","code_tokenizer.train_from_iterator(y, trainer=trainer)\n","code_tokenizer.post_processor = processors.BertProcessing(('[CLS]',code_tokenizer.token_to_id('[CLS]')),('[SEP]',code_tokenizer.token_to_id('[SEP]')))"]},{"cell_type":"code","source":["nl_tokenizer.save(f'{path}/nl_tokenizer.json')\n","code_tokenizer.save(f'{path}/code_tokenizer.json')"],"metadata":{"id":"vIbcj1Hhr_kQ","executionInfo":{"status":"ok","timestamp":1675954968471,"user_tz":-60,"elapsed":369,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kivsx9FrRat"},"outputs":[],"source":["filter_x = np.array(list(map(lambda item: item.ids, nl_tokenizer.encode_batch(x))))\n","filter_y = np.array(list(map(lambda item: item.ids, code_tokenizer.encode_batch(y))))\n","length=400\n","nl_tokenizer.enable_padding()\n","code_tokenizer.enable_padding()\n","x=x[(np.array([len(i) for i in filter_y])<length) & (np.array([len(i) for i in filter_x])<length)]\n","y=y[(np.array([len(i) for i in filter_y])<length) & (np.array([len(i) for i in filter_x])<length)]\n","del filter_x\n","del filter_y\n","x_tokenized=np.array(list(map(lambda t: t.ids, nl_tokenizer.encode_batch(x))))\n","y_tokenized=np.array(list(map(lambda t: t.ids, code_tokenizer.encode_batch(y))))\n","x_tokenized.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nt8WnQygrrV1","outputId":"9a051e6e-f80c-4427-d9b8-68223bde82f9"},"outputs":[{"data":{"text/plain":["(8000, 8000)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_test, y_train, y_test = train_test_split(x_tokenized, y_tokenized, test_size=0.2, random_state=42)\n","X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n","\n","np.save(f'{path}/X_train.npy', X_train)\n","np.save(f'{path}/y_train.npy', y_train)\n","np.save(f'{path}/X_test.npy', X_test)\n","np.save(f'{path}/y_test.npy', y_test)\n","np.save(f'{path}/X_val.npy', X_val)\n","np.save(f'{path}/y_val.npy', y_val)\n","\n","input_vocab_size=nl_tokenizer.get_vocab_size()\n","output_vocab_size=code_tokenizer.get_vocab_size()\n","input_vocab_size,output_vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pP-eSirTsO2u","outputId":"88098de9-8cb3-4ef2-b09d-2053401bb3e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," masking_4 (Masking)         (None, 398, 1)            0         \n","                                                                 \n"," bidirectional_4 (Bidirectio  (None, 796)              1273600   \n"," nal)                                                            \n","                                                                 \n"," repeat_vector_4 (RepeatVect  (None, 399, 796)         0         \n"," or)                                                             \n","                                                                 \n"," lstm_15 (LSTM)              (None, 399, 512)          2680832   \n","                                                                 \n"," lstm_16 (LSTM)              (None, 399, 1024)         6295552   \n","                                                                 \n"," dropout_8 (Dropout)         (None, 399, 1024)         0         \n","                                                                 \n"," time_distributed_8 (TimeDis  (None, 399, 1024)        1049600   \n"," tributed)                                                       \n","                                                                 \n"," dropout_9 (Dropout)         (None, 399, 1024)         0         \n","                                                                 \n"," time_distributed_9 (TimeDis  (None, 399, 8000)        8200000   \n"," tributed)                                                       \n","                                                                 \n","=================================================================\n","Total params: 19,499,584\n","Trainable params: 19,499,584\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["input_length=X_train.shape[1]\n","output_length=y_train.shape[1]\n","model = Sequential([\n","Masking(mask_value=0,input_shape=(input_length, 1)),\n","Bidirectional(LSTM(input_length, return_sequences=False)),\n","RepeatVector(output_length),\n","LSTM(512, return_sequences=True),\n","LSTM(1024, return_sequences=True),\n","Dropout(0.2),\n","TimeDistributed(Dense(1024)),\n","Dropout(0.2),\n","TimeDistributed(Dense(units=output_vocab_size))\n","])\n","model.compile()\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Nq2_LxauNmY"},"outputs":[],"source":["def loss_function(x, y):\n","  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","  loss = cross_entropy(y_true=y, y_pred=x)\n","  mask = tf.logical_not(tf.math.equal(y,0))\n","  mask = tf.cast(mask, dtype=loss.dtype)  \n","  loss = mask* loss\n","  loss = tf.reduce_mean(loss)\n","  return loss\n","\n","buffer_size=32000\n","batch_size=150\n","Y_len = np.count_nonzero(y_train, axis=1)\n","train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train, Y_len)).shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n","valid_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val, np.count_nonzero(y_val, axis=1))).shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n","optimizer = tf.keras.optimizers.Adam()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vi2KPFnlrRau"},"outputs":[],"source":["for epoch in range(5000):\n","  avg_loss = 0\n","  training_step = 0\n","  for x_train2, y_train2, data_len in train_ds:\n","      with tf.GradientTape() as tape:\n","          loss = loss_function(model(x_train2), y_train2)\n","      grads = tape.gradient(loss, model.trainable_variables)\n","      optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n","      avg_loss += loss\n","      training_step += 1\n","  avg_loss /= training_step\n","  \n","  if (epoch + 1) % 10 == 0:\n","      avg_val_loss = 0\n","      val_training_step = 0\n","      for x_valid2, y_valid2, data_len2 in valid_ds:\n","          val_loss = loss_function(model(x_valid2), y_valid2)\n","          avg_val_loss += loss\n","          val_training_step += 1\n","      avg_val_loss /= val_training_step\n","      print('val_loss: {:.3f}'.format(avg_val_loss))\n","      print('Epoch: {:3}, tr_loss: {:.3f}'.format((epoch+1)/100, avg_loss))\n","      model.save(f'{path}/c_model.h5')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xmdDoxJPuetw","outputId":"3b5960a7-2f8e-4f96-f9f6-1d48ebe785d7","executionInfo":{"status":"ok","timestamp":1675955111893,"user_tz":-60,"elapsed":6738,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["find the area of a regular nk polygon inscribed in a circle of radius 1 however a regular nk polygon is take n points at equal intervals on the circumference the outermost figure connecting every k-1 points defined as for example a 52 polygon can be drawn as follows first take five equally spaceh points on a circle of radius 1 then connect each point every 2-1=1 the outermost figure is a regular 52 polygon\n","# include < stdio.h > # include < stdlib.h { int n for n ( i, 0, i, s ; long long \" & \", \", ) ; for ( i ) a int = = 0 a \", ( \" \" \" ( ( a \" ( & < < ) scanf ( \" < 0 ; i < a ) i - \" + ) { \" ( \" ) n n n ) i ) ( ( 0 ) { ++ ( ( ) ) ( d ) ) ; ; d ; ; } ; } ) ( \" ) 0 ) ; } ) 0 0 } } } } } } } ( \" \" ) ) ) ) ; ; ; ; ; ; ; ; ; ; ; ; ; ; ) ) ) ) ( ( ( * * * ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) d d d d d d d d d d d d d d d d d d d d d d d ) ) ) \" \" \" \" \" \" \" \" \" \" \" \" \" \" ) ) ) ) ; ; ; } } } } } } } 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }\n","// // # include < stdio.h > # include < math.h > # define pi 3.1415926535897932384626433832795 // int main ( ) { int n, k ; scanf ( \" %d%d \", & n, & k ) ; printf ( \"wise \", n * sin ( pg ) * cos ( k * pg ) cos ( ( k - 1 ) * pg ) ) ; return 0 ; }\n"]}],"source":["sample_num=12\n","y_pred = model.predict(np.expand_dims(X_test[sample_num],axis=0),verbose=0)\n","y_pred = np.argmax(y_pred, axis=-1) \n","print(nl_tokenizer.decode(X_test[sample_num]))\n","print(code_tokenizer.decode(y_pred[0]))\n","print(code_tokenizer.decode(y_test[sample_num]))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Is2jjdPYuhr-","outputId":"c8e1c0db-beb5-4684-aa4b-74b2c5d26dca","executionInfo":{"status":"ok","timestamp":1675955118768,"user_tz":-60,"elapsed":2785,"user":{"displayName":"Jan Kowalski","userId":"01688604514757342946"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["you are given an integer sequence of length n a1 an let us consider performing the following n operations on an empty sequence b the i-th operation is as follows becoming ai to the end of b reverse the order of the elements in b find the sequence b obtained after these n operations\n","i ; b [ 1 << 18 ] ; j ; j ( n, a ) { for ( i ~ scanf ( \" %d \", & n ) - n % 2 ; ~ scanf ( \" %d \", & a ) ; ) b [ ( n - i ) 2 + i % 2 * i ++ ] = a ; for for j j j < n ; ) printf ( \" %d \", b [ j - 1 1 ) ; } } \" \" )\n","i ; b [ 1 << 18 ] ; j ; main ( n, a ) { for ( i = scanf ( \" %d \", & n ) - n % 2 ; ~ scanf ( \" %d \", & a ) ; ) b [ ( n - i ) 2 + i % 2 * i ++ ] = a ; for ( ; j ++ < n ; ) printf ( \" %d \", b [ j - 1 ] ) ; }\n"]}],"source":["sample_num=0\n","y_pred = model.predict(np.expand_dims(X_train[sample_num],axis=0),verbose=0)\n","y_pred = np.argmax(y_pred, axis=-1)\n","print(nl_tokenizer.decode(X_train[sample_num]))\n","print(code_tokenizer.decode(y_pred[0]))\n","print(code_tokenizer.decode(y_train[sample_num]))"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}