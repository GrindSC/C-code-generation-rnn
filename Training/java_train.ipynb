{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35367,"status":"ok","timestamp":1671201036967,"user":{"displayName":"Filip Cze","userId":"04946656448088677164"},"user_tz":-60},"id":"ddzJXkdDEtOB","outputId":"746e96cc-bb6d-4786-bf2b-819d950dbef6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K     |████████████████████████████████| 7.6 MB 11.2 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 9.8 MB/s \n","\u001b[K     |████████████████████████████████| 182 kB 50.8 MB/s \n","\u001b[?25h"]}],"source":["!pip install -q tokenizers\n","!pip install -q transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25175,"status":"ok","timestamp":1671201062139,"user":{"displayName":"Filip Cze","userId":"04946656448088677164"},"user_tz":-60},"id":"Q07NJLEpEymr","outputId":"83577459-65b6-44ff-c52d-d6e409e0dd5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM, Embedding, Masking\n","from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers, processors\n","from sklearn.model_selection import train_test_split\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZm5jKh_FXSx"},"outputs":[],"source":["url = 'https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Text-Code/text-to-code/dataset/concode/train.json'\n","train = pd.read_json(url,  dtype='dict',lines=True)\n","train=train[['nl','code']]\n","x=train['nl'].to_numpy()\n","y=train['code'].to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0WrHF8KE-jG"},"outputs":[],"source":["nl_tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\n","nl_tokenizer.normalizer = normalizers.BertNormalizer(clean_text = False)\n","nl_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n","nl_tokenizer.decoder = decoders.WordPiece(prefix='##')\n","trainer = trainers.WordPieceTrainer(\n","    vocab_size=8000,\n","    show_progress=True,\n","    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n","    continuing_subword_prefix='##'\n",")\n","nl_tokenizer.train_from_iterator(x, trainer=trainer)\n","nl_tokenizer.post_processor = processors.BertProcessing(('[CLS]',nl_tokenizer.token_to_id('[CLS]')),('[SEP]',nl_tokenizer.token_to_id('[SEP]')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzJD7nSNGlih"},"outputs":[],"source":["code_tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\n","code_tokenizer.normalizer = normalizers.BertNormalizer(clean_text = False)\n","code_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n","code_tokenizer.decoder = decoders.WordPiece(prefix='##')\n","trainer = trainers.WordPieceTrainer(\n","    vocab_size=8000,\n","    show_progress=True,\n","    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n","    continuing_subword_prefix='##'\n",")\n","code_tokenizer.train_from_iterator(y, trainer=trainer)\n","code_tokenizer.post_processor = processors.BertProcessing(('[CLS]',code_tokenizer.token_to_id('[CLS]')),('[SEP]',code_tokenizer.token_to_id('[SEP]')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7IxratLlFTn8","outputId":"03ba18c2-1378-4530-9264-94db3932780e"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-7-b2004afb1b87>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  tokenized_data=np.array(list(map(lambda item: item.ids, nl_tokenizer.encode_batch(x))))\n"]}],"source":["tokenized_data=np.array(list(map(lambda item: item.ids, nl_tokenizer.encode_batch(x))))\n","\n","nl_tokenizer.enable_padding()\n","code_tokenizer.enable_padding()\n","x=x[np.array([len(i) for i in tokenized_data])<150]\n","y=y[np.array([len(i) for i in tokenized_data])<150]\n","x_tokenized=np.array(list(map(lambda t: t.ids, nl_tokenizer.encode_batch(x))))\n","y_tokenized=np.array(list(map(lambda t: t.ids, code_tokenizer.encode_batch(y))))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-UUU0tjGyuz","outputId":"c7251798-2a59-4f3b-d9d4-f2337e180923"},"outputs":[{"data":{"text/plain":["(8000, 8000)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_test, y_train, y_test = train_test_split(x_tokenized, y_tokenized, test_size=0.2, random_state=42)\n","X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n","\n","path='/Data/java_train'\n","np.save(f'{path}/X_train.npy', X_train)\n","np.save(f'{path}/y_train.npy', y_train)\n","np.save(f'{path}/X_test.npy', X_test)\n","np.save(f'{path}/y_test.npy', y_test)\n","np.save(f'{path}/X_val.npy', X_val)\n","np.save(f'{path}/y_val.npy', y_val)\n","\n","input_vocab_size=nl_tokenizer.get_vocab_size()\n","output_vocab_size=code_tokenizer.get_vocab_size()\n","input_vocab_size,output_vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCF492Do9bjM"},"outputs":[],"source":["nl_tokenizer.save('/Data/java_train/nl_tokenizer.json')\n","code_tokenizer.save('/Data/java_train/code_tokenizer.json')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6140,"status":"ok","timestamp":1671201121994,"user":{"displayName":"Filip Cze","userId":"04946656448088677164"},"user_tz":-60},"id":"iywuqUFzV3qc","outputId":"5370774f-ce00-49e0-9726-8429580bd4d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," masking (Masking)           (None, 149, 1)            0         \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 298)              179992    \n"," l)                                                              \n","                                                                 \n"," repeat_vector (RepeatVector  (None, 153, 298)         0         \n"," )                                                               \n","                                                                 \n"," lstm_1 (LSTM)               (None, 153, 512)          1660928   \n","                                                                 \n"," lstm_2 (LSTM)               (None, 153, 1024)         6295552   \n","                                                                 \n"," dropout (Dropout)           (None, 153, 1024)         0         \n","                                                                 \n"," time_distributed (TimeDistr  (None, 153, 1024)        1049600   \n"," ibuted)                                                         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 153, 1024)         0         \n","                                                                 \n"," time_distributed_1 (TimeDis  (None, 153, 8000)        8200000   \n"," tributed)                                                       \n","                                                                 \n","=================================================================\n","Total params: 17,386,072\n","Trainable params: 17,386,072\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["input_length=X_train.shape[1]\n","output_length=y_train.shape[1]\n","model = Sequential([\n","Masking(mask_value=0,input_shape=(input_length, 1)),\n","Bidirectional(LSTM(input_length, return_sequences=False)),\n","RepeatVector(output_length),\n","LSTM(512, return_sequences=True),\n","LSTM(1024, return_sequences=True),\n","Dropout(0.2),\n","TimeDistributed(Dense(1024)),\n","Dropout(0.2),\n","TimeDistributed(Dense(units=output_vocab_size))\n","])\n","model.compile()\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXHgUrkgIhae"},"outputs":[],"source":["def loss_function(x, y):\n","  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","  loss = cross_entropy(y_true=y, y_pred=x)\n","  mask = tf.logical_not(tf.math.equal(y,0))\n","  mask = tf.cast(mask, dtype=loss.dtype)  \n","  loss = mask* loss\n","  loss = tf.reduce_mean(loss)\n","  return loss\n","\n","buffer_size=32000\n","batch_size=240\n","Y_len = np.count_nonzero(y_train, axis=1)\n","train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train, Y_len)).shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n","valid_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val, np.count_nonzero(y_val, axis=1))).shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n","optimizer = tf.keras.optimizers.Adam()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Afd-LQm_WwgE"},"outputs":[],"source":["for epoch in range(5000):\n","  avg_loss = 0\n","  training_step = 0\n","  for x_train2, y_train2, data_len in train_ds:\n","      with tf.GradientTape() as tape:\n","          loss = loss_function(model(x_train2), y_train2)\n","      grads = tape.gradient(loss, model.trainable_variables)\n","      optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n","      avg_loss += loss\n","      training_step += 1\n","  avg_loss /= training_step\n","  \n","  if (epoch + 1) % 10 == 0:\n","      avg_val_loss = 0\n","      val_training_step = 0\n","      for x_valid2, y_valid2, data_len2 in valid_ds:\n","          val_loss = loss_function(model(x_valid2), y_valid2)\n","          avg_val_loss += loss\n","          val_training_step += 1\n","      avg_val_loss /= val_training_step\n","      print('val_loss: {:.3f}'.format(avg_val_loss))\n","      print('Epoch: {:3}, tr_loss: {:.3f}'.format((epoch+1)/100, avg_loss))\n","      model.save('/Data/java_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9wTJKb1MvUu","outputId":"9cedd469-e28e-4dd6-a08e-28b2b1cb75ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["sets the iteration number. concode_field_sep int n_i concode_elem_sep int tau concode_elem_sep long serialversionuid concode_elem_sep double counter concode_elem_sep int tau_t concode_elem_sep int m concode_elem_sep int n concode_field_sep void setn_t concode_elem_sep int getn_t concode_elem_sep double apply concode_elem_sep double apply concode_elem_sep int getm concode_elem_sep void settau_t concode_elem_sep int getn concode_elem_sep void setn concode_elem_sep int gettau_t concode_elem_sep int gettau concode_elem_sep void setm\n","void function ( ) { return ( ;. size. arg0 ; } if ( loc0 ; size ; } payload ; } ; ; ) payload ; payload ( put ( arg0 ) ; else else else else else ) else else else else else else else else else else else else else else else else } } } } } } } } } } } } } } } } }\n","void function ( int arg0 ) { this. tdet = arg0 ; }\n"]}],"source":["sample_num=12\n","y_pred = model.predict(np.expand_dims(X_test[sample_num],axis=0),verbose=0)\n","y_pred = np.argmax(y_pred, axis=-1) \n","print(nl_tokenizer.decode(X_test[sample_num]))\n","print(code_tokenizer.decode(y_pred[0]))\n","print(code_tokenizer.decode(y_test[sample_num]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BpGbG0wdMwr2","outputId":"7b307099-5906-4924-fcc0-97ea7d173d6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["thid method closes the buffered reader concode_field_sep string log_arcwivemun concode_field_sep bufferedwriter getnewfilewriter concode_elem_septemplatelink getnewstreamreader\n","void function ( bufferedreader arg0 ) { if ( arg0!= null ) { try ( arg0. arg0 ( ) ; } catch. ioexception ( ) { loc0. loc0 ( ) ) ; } ; }.....ext ( ) ) ) ) ) ) ) ) ) ) ) ) } } } } } } } } }\n","void function ( bufferedreader arg0 ) { if ( arg0!= null ) { try { arg0. close ( ) ; } catch ( ioexception loc0 ) { loc0. printstacktrace ( ) ; } } }\n"]}],"source":["sample_num=0\n","y_pred = model.predict(np.expand_dims(X_train[sample_num],axis=0),verbose=0)\n","y_pred = np.argmax(y_pred, axis=-1)\n","print(nl_tokenizer.decode(X_train[sample_num]))\n","print(code_tokenizer.decode(y_pred[0]))\n","print(code_tokenizer.decode(y_train[sample_num]))"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
